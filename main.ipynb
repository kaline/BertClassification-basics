{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação com Bert\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/text/tutorials/classify_text_with_bert\">Tutorial link</a>\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Load the IMDB dataset\n",
    "- Load a BERT model from TensorFlow Hub\n",
    "- Build your own model by combining BERT with a classifier\n",
    "- Train your own model, fine-tuning BERT as part of that\n",
    "- Save your model and use it to classify sentences\n",
    "If you're new to working with the IMDB dataset, please see Basic text classification for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "! pip install -q -U \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/kaline/anaconda3/bin/python /home/kaline/anaconda3/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /tmp/tmpl4u35vbu\n",
      "       cwd: /tmp/pip-install-fjsc0lls/pycocotools\n",
      "  Complete output (20 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-cpython-37\n",
      "  creating build/lib.linux-x86_64-cpython-37/pycocotools\n",
      "  copying pycocotools/coco.py -> build/lib.linux-x86_64-cpython-37/pycocotools\n",
      "  copying pycocotools/__init__.py -> build/lib.linux-x86_64-cpython-37/pycocotools\n",
      "  copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-cpython-37/pycocotools\n",
      "  copying pycocotools/mask.py -> build/lib.linux-x86_64-cpython-37/pycocotools\n",
      "  running build_ext\n",
      "  cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
      "  /tmp/pip-build-env-kmvpd3eg/overlay/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /tmp/pip-install-fjsc0lls/pycocotools/pycocotools/_mask.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  building 'pycocotools._mask' extension\n",
      "  creating build/temp.linux-x86_64-cpython-37\n",
      "  creating build/temp.linux-x86_64-cpython-37/common\n",
      "  creating build/temp.linux-x86_64-cpython-37/pycocotools\n",
      "  gcc -pthread -B /home/kaline/anaconda3/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/tmp/pip-build-env-kmvpd3eg/overlay/lib/python3.7/site-packages/numpy/core/include -I./common -I/home/kaline/anaconda3/include/python3.7m -c ./common/maskApi.c -o build/temp.linux-x86_64-cpython-37/./common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "  error: command 'gcc' failed: No such file or directory: 'gcc'\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for pycocotools\u001b[0m\n",
      "\u001b[31mERROR: Could not build wheels for pycocotools which use PEP 517 and cannot be installed directly\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q tf-models-official==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise de sentimentos\n",
    "\n",
    "Este notebook treina um modelo de análise de sentimentos para classificar avaliações de filmes como positivas ou negativas, baseadas no texto da revisão.\n",
    "Você vai usar <a href=\"https://ai.stanford.edu/~amaas/data/sentiment/\">Large Movie Review Dataset</a>, no qual contém o texto de 50,000 avaliações de filmes do <a href=\"https://www.imdb.com/\">Internet Movie database</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download o IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar o dataset e explorar a estrutura do diretório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url, untar=True, cache_dir='.', cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove pastas não utilizadas para ser mais fácil carregar os dados\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na próxima, você vai utilizar a utilidade text_dataset_from_directory para criar o rotulado tf.data.Dataset\n",
    "\n",
    "O IMDB dataset já foi dividido em treino e teste, mas necessita um conjunto de validação. Vamos criar um conjunto de validação usando a divisão 80:20 do dados de treinamento usando o parâmetro validation_split abaixo.\n",
    "\n",
    "Obs: necessário especificar um seed ou shuffle=False, assim o conjunto de validação e treinamento não terão sobreposição (overlap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28b8b178ce56687a460bda9a4b177d23f3f80c348f170196c623443fb6ec97fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
